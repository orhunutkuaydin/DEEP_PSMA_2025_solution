
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-08-16 13:35:22.712085: Using torch.compile... 
2025-08-16 13:35:25.335406: do_dummy_2d_data_aug: False 

This is the configuration used by this training:
Configuration name: 3d_fullres
 {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [192, 192, 192], 'median_image_size_in_voxels': [192.0, 335.0, 192.0], 'spacing': [3.6458332538604736, 3.2699999809265137, 3.6458332538604736], 'normalization_schemes': ['NoNormalization', 'CTNormalization'], 'use_mask_for_norm': [False, False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.ResidualEncoderUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 1]], 'n_blocks_per_stage': [1, 3, 4, 6, 6, 6], 'n_conv_per_stage_decoder': [1, 1, 1, 1, 1], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset800_AUTO_PET', 'plans_name': 'nnUNetResEncUNetMPlans_192', 'original_median_spacing_after_transp': [3.6458332538604736, 3.2699999809265137, 3.6458332538604736], 'original_median_shape_after_transp': [192, 335, 192], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [1, 0, 2], 'transpose_backward': [1, 0, 2], 'experiment_planner_used': 'nnUNetPlannerResEncM', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 122.71910858154297, 'mean': 2.513272523880005, 'median': 1.7651877403259277, 'min': 0.9490564465522766, 'percentile_00_5': 1.0040589570999146, 'percentile_99_5': 18.182958602905273, 'std': 2.67779803276062}, '1': {'max': 3071.0, 'mean': 61.71842575073242, 'median': 37.0, 'min': -2352.0, 'percentile_00_5': -625.0, 'percentile_99_5': 1006.0, 'std': 181.78611755371094}}} 
 
2025-08-16 13:35:26.852003: Unable to plot network architecture: nnUNet_compile is enabled! 
2025-08-16 13:35:26.861170:  
2025-08-16 13:35:26.861395: Epoch 950 
2025-08-16 13:35:26.861559: Current learning rate: 0.00067 
2025-08-16 13:38:27.995589: train_loss -0.8873 
2025-08-16 13:38:27.995682: val_loss -0.908 
2025-08-16 13:38:27.995714: Pseudo dice [np.float32(0.9339), np.float32(0.9649)] 
2025-08-16 13:38:27.995769: Epoch time: 181.14 s 
2025-08-16 13:38:27.995795: Yayy! New best EMA pseudo Dice: 0.9488000273704529 
2025-08-16 13:38:29.094230:  
2025-08-16 13:38:29.094299: Epoch 951 
2025-08-16 13:38:29.094350: Current learning rate: 0.00066 
2025-08-16 13:40:31.576116: train_loss -0.8852 
2025-08-16 13:40:31.576218: val_loss -0.911 
2025-08-16 13:40:31.576244: Pseudo dice [np.float32(0.9277), np.float32(0.9659)] 
2025-08-16 13:40:31.576290: Epoch time: 122.48 s 
2025-08-16 13:40:32.040353:  
2025-08-16 13:40:32.040441: Epoch 952 
2025-08-16 13:40:32.040497: Current learning rate: 0.00065 
2025-08-16 13:42:34.466570: train_loss -0.8901 
2025-08-16 13:42:34.466673: val_loss -0.894 
2025-08-16 13:42:34.466700: Pseudo dice [np.float32(0.9271), np.float32(0.9655)] 
2025-08-16 13:42:34.466732: Epoch time: 122.43 s 
2025-08-16 13:42:34.929016:  
2025-08-16 13:42:34.929104: Epoch 953 
2025-08-16 13:42:34.929154: Current learning rate: 0.00064 
2025-08-16 13:44:37.345023: train_loss -0.8863 
2025-08-16 13:44:37.345119: val_loss -0.9097 
2025-08-16 13:44:37.345145: Pseudo dice [np.float32(0.934), np.float32(0.9636)] 
2025-08-16 13:44:37.353618: Epoch time: 122.42 s 
2025-08-16 13:44:38.253718:  
2025-08-16 13:44:38.253840: Epoch 954 
2025-08-16 13:44:38.253893: Current learning rate: 0.00063 
2025-08-16 13:46:40.759560: train_loss -0.8937 
2025-08-16 13:46:40.759662: val_loss -0.906 
2025-08-16 13:46:40.759689: Pseudo dice [np.float32(0.9332), np.float32(0.9641)] 
2025-08-16 13:46:40.759721: Epoch time: 122.51 s 
2025-08-16 13:46:41.224761:  
2025-08-16 13:46:41.224847: Epoch 955 
2025-08-16 13:46:41.224899: Current learning rate: 0.00061 
2025-08-16 13:48:43.691787: train_loss -0.8944 
2025-08-16 13:48:43.691881: val_loss -0.9069 
2025-08-16 13:48:43.691908: Pseudo dice [np.float32(0.9271), np.float32(0.9619)] 
2025-08-16 13:48:43.691939: Epoch time: 122.47 s 
2025-08-16 13:48:44.155880:  
2025-08-16 13:48:44.155960: Epoch 956 
2025-08-16 13:48:44.156011: Current learning rate: 0.0006 
2025-08-16 13:50:49.082477: train_loss -0.8901 
2025-08-16 13:50:49.082573: val_loss -0.9032 
2025-08-16 13:50:49.082599: Pseudo dice [np.float32(0.9345), np.float32(0.967)] 
2025-08-16 13:50:49.082630: Epoch time: 124.93 s 
2025-08-16 13:50:49.547482:  
2025-08-16 13:50:49.547559: Epoch 957 
2025-08-16 13:50:49.547610: Current learning rate: 0.00059 
2025-08-16 13:52:52.065014: train_loss -0.8931 
2025-08-16 13:52:52.065110: val_loss -0.9187 
2025-08-16 13:52:52.065136: Pseudo dice [np.float32(0.9349), np.float32(0.9645)] 
2025-08-16 13:52:52.065172: Epoch time: 122.52 s 
2025-08-16 13:52:52.530571:  
2025-08-16 13:52:52.530739: Epoch 958 
2025-08-16 13:52:52.530803: Current learning rate: 0.00058 
2025-08-16 13:54:55.112548: train_loss -0.8906 
2025-08-16 13:54:55.112647: val_loss -0.9003 
2025-08-16 13:54:55.112672: Pseudo dice [np.float32(0.9277), np.float32(0.9643)] 
2025-08-16 13:54:55.112704: Epoch time: 122.58 s 
2025-08-16 13:54:55.584455:  
2025-08-16 13:54:55.584542: Epoch 959 
2025-08-16 13:54:55.584593: Current learning rate: 0.00056 
2025-08-16 13:56:58.060955: train_loss -0.8969 
2025-08-16 13:56:58.069453: val_loss -0.9053 
2025-08-16 13:56:58.069481: Pseudo dice [np.float32(0.9291), np.float32(0.9632)] 
2025-08-16 13:56:58.069511: Epoch time: 122.48 s 
2025-08-16 13:56:58.535327:  
2025-08-16 13:56:58.535409: Epoch 960 
2025-08-16 13:56:58.535462: Current learning rate: 0.00055 
2025-08-16 13:59:01.024999: train_loss -0.8954 
2025-08-16 13:59:01.025098: val_loss -0.9124 
2025-08-16 13:59:01.025125: Pseudo dice [np.float32(0.9313), np.float32(0.9678)] 
2025-08-16 13:59:01.025160: Epoch time: 122.49 s 
2025-08-16 13:59:01.487934:  
2025-08-16 13:59:01.488020: Epoch 961 
2025-08-16 13:59:01.488072: Current learning rate: 0.00054 
2025-08-16 14:01:04.003235: train_loss -0.8927 
2025-08-16 14:01:04.003332: val_loss -0.9192 
2025-08-16 14:01:04.003358: Pseudo dice [np.float32(0.9346), np.float32(0.9658)] 
2025-08-16 14:01:04.003389: Epoch time: 122.52 s 
2025-08-16 14:01:04.467136:  
2025-08-16 14:01:04.467214: Epoch 962 
2025-08-16 14:01:04.467267: Current learning rate: 0.00053 
2025-08-16 14:03:06.956169: train_loss -0.8958 
2025-08-16 14:03:06.956261: val_loss -0.9095 
2025-08-16 14:03:06.964801: Pseudo dice [np.float32(0.9372), np.float32(0.9651)] 
2025-08-16 14:03:06.964862: Epoch time: 122.49 s 
2025-08-16 14:03:07.430991:  
2025-08-16 14:03:07.431066: Epoch 963 
2025-08-16 14:03:07.431122: Current learning rate: 0.00051 
2025-08-16 14:05:09.959650: train_loss -0.8945 
2025-08-16 14:05:09.959741: val_loss -0.8837 
2025-08-16 14:05:09.959769: Pseudo dice [np.float32(0.9349), np.float32(0.9645)] 
2025-08-16 14:05:09.959809: Epoch time: 122.53 s 
2025-08-16 14:05:10.423352:  
2025-08-16 14:05:10.423425: Epoch 964 
2025-08-16 14:05:10.423477: Current learning rate: 0.0005 
2025-08-16 14:07:12.899024: train_loss -0.8926 
2025-08-16 14:07:12.899131: val_loss -0.9022 
2025-08-16 14:07:12.899156: Pseudo dice [np.float32(0.9344), np.float32(0.9642)] 
2025-08-16 14:07:12.899189: Epoch time: 122.48 s 
2025-08-16 14:07:12.899215: Yayy! New best EMA pseudo Dice: 0.9488000273704529 
2025-08-16 14:07:14.034499:  
2025-08-16 14:07:14.034588: Epoch 965 
2025-08-16 14:07:14.034639: Current learning rate: 0.00049 
2025-08-16 14:09:16.486020: train_loss -0.9002 
2025-08-16 14:09:16.486117: val_loss -0.9122 
2025-08-16 14:09:16.486142: Pseudo dice [np.float32(0.933), np.float32(0.9674)] 
2025-08-16 14:09:16.486173: Epoch time: 122.45 s 
2025-08-16 14:09:16.486193: Yayy! New best EMA pseudo Dice: 0.9488999843597412 
2025-08-16 14:09:17.603378:  
2025-08-16 14:09:17.603450: Epoch 966 
2025-08-16 14:09:17.603501: Current learning rate: 0.00048 
2025-08-16 14:11:21.609400: train_loss -0.8903 
2025-08-16 14:11:21.609520: val_loss -0.8881 
2025-08-16 14:11:21.609547: Pseudo dice [np.float32(0.9322), np.float32(0.9654)] 
2025-08-16 14:11:21.609579: Epoch time: 124.01 s 
2025-08-16 14:11:22.081522:  
2025-08-16 14:11:22.081605: Epoch 967 
2025-08-16 14:11:22.081663: Current learning rate: 0.00046 
2025-08-16 14:13:26.102748: train_loss -0.8983 
2025-08-16 14:13:26.102844: val_loss -0.9106 
2025-08-16 14:13:26.102872: Pseudo dice [np.float32(0.9352), np.float32(0.9645)] 
2025-08-16 14:13:26.102911: Epoch time: 124.02 s 
2025-08-16 14:13:26.102933: Yayy! New best EMA pseudo Dice: 0.9490000009536743 
2025-08-16 14:13:27.231683:  
2025-08-16 14:13:27.231750: Epoch 968 
2025-08-16 14:13:27.231800: Current learning rate: 0.00045 
2025-08-16 14:15:29.775425: train_loss -0.8901 
2025-08-16 14:15:29.775530: val_loss -0.8968 
2025-08-16 14:15:29.775566: Pseudo dice [np.float32(0.9336), np.float32(0.9651)] 
2025-08-16 14:15:29.775604: Epoch time: 122.55 s 
2025-08-16 14:15:29.775630: Yayy! New best EMA pseudo Dice: 0.9491000175476074 
2025-08-16 14:15:30.894484:  
2025-08-16 14:15:30.894562: Epoch 969 
2025-08-16 14:15:30.894615: Current learning rate: 0.00044 
2025-08-16 14:17:33.457988: train_loss -0.8938 
2025-08-16 14:17:33.458102: val_loss -0.9033 
2025-08-16 14:17:33.458128: Pseudo dice [np.float32(0.9361), np.float32(0.9654)] 
2025-08-16 14:17:33.458165: Epoch time: 122.56 s 
2025-08-16 14:17:33.458186: Yayy! New best EMA pseudo Dice: 0.9491999745368958 
2025-08-16 14:17:34.929949:  
2025-08-16 14:17:34.930021: Epoch 970 
2025-08-16 14:17:34.930071: Current learning rate: 0.00043 
2025-08-16 14:19:37.513616: train_loss -0.9017 
2025-08-16 14:19:37.513728: val_loss -0.9188 
2025-08-16 14:19:37.513754: Pseudo dice [np.float32(0.9357), np.float32(0.9685)] 
2025-08-16 14:19:37.513800: Epoch time: 122.58 s 
2025-08-16 14:19:37.522342: Yayy! New best EMA pseudo Dice: 0.9495000243186951 
2025-08-16 14:19:38.653603:  
2025-08-16 14:19:38.653677: Epoch 971 
2025-08-16 14:19:38.653727: Current learning rate: 0.00041 
2025-08-16 14:21:41.205464: train_loss -0.8933 
2025-08-16 14:21:41.205556: val_loss -0.9065 
2025-08-16 14:21:41.205582: Pseudo dice [np.float32(0.9348), np.float32(0.9647)] 
2025-08-16 14:21:41.205612: Epoch time: 122.55 s 
2025-08-16 14:21:41.205632: Yayy! New best EMA pseudo Dice: 0.9495000243186951 
2025-08-16 14:21:42.330654:  
2025-08-16 14:21:42.330730: Epoch 972 
2025-08-16 14:21:42.330782: Current learning rate: 0.0004 
2025-08-16 14:23:45.402374: train_loss -0.8929 
2025-08-16 14:23:45.402468: val_loss -0.9174 
2025-08-16 14:23:45.402495: Pseudo dice [np.float32(0.937), np.float32(0.9682)] 
2025-08-16 14:23:45.402526: Epoch time: 123.07 s 
2025-08-16 14:23:45.402550: Yayy! New best EMA pseudo Dice: 0.9498000144958496 
2025-08-16 14:23:46.526115:  
2025-08-16 14:23:46.526192: Epoch 973 
2025-08-16 14:23:46.526248: Current learning rate: 0.00039 
2025-08-16 14:25:49.221050: train_loss -0.9008 
2025-08-16 14:25:49.221154: val_loss -0.9048 
2025-08-16 14:25:49.221180: Pseudo dice [np.float32(0.9358), np.float32(0.9688)] 
2025-08-16 14:25:49.221210: Epoch time: 122.7 s 
2025-08-16 14:25:49.221232: Yayy! New best EMA pseudo Dice: 0.9501000046730042 
2025-08-16 14:25:50.347253:  
2025-08-16 14:25:50.347337: Epoch 974 
2025-08-16 14:25:50.347390: Current learning rate: 0.00037 
2025-08-16 14:27:52.877512: train_loss -0.8995 
2025-08-16 14:27:52.877613: val_loss -0.9086 
2025-08-16 14:27:52.877641: Pseudo dice [np.float32(0.932), np.float32(0.9634)] 
2025-08-16 14:27:52.877673: Epoch time: 122.53 s 
2025-08-16 14:27:53.343219:  
2025-08-16 14:27:53.343293: Epoch 975 
2025-08-16 14:27:53.343344: Current learning rate: 0.00036 
2025-08-16 14:29:55.869246: train_loss -0.8911 
2025-08-16 14:29:55.869339: val_loss -0.894 
2025-08-16 14:29:55.869369: Pseudo dice [np.float32(0.9302), np.float32(0.9656)] 
2025-08-16 14:29:55.869405: Epoch time: 122.53 s 
2025-08-16 14:29:56.334812:  
2025-08-16 14:29:56.334902: Epoch 976 
2025-08-16 14:29:56.334955: Current learning rate: 0.00035 
2025-08-16 14:31:58.867012: train_loss -0.8933 
2025-08-16 14:31:58.867112: val_loss -0.8932 
2025-08-16 14:31:58.867138: Pseudo dice [np.float32(0.9287), np.float32(0.9674)] 
2025-08-16 14:31:58.867167: Epoch time: 122.53 s 
2025-08-16 14:31:59.332780:  
2025-08-16 14:31:59.332864: Epoch 977 
2025-08-16 14:31:59.332917: Current learning rate: 0.00034 
2025-08-16 14:34:01.889499: train_loss -0.894 
2025-08-16 14:34:01.889586: val_loss -0.904 
2025-08-16 14:34:01.889611: Pseudo dice [np.float32(0.9372), np.float32(0.9652)] 
2025-08-16 14:34:01.889643: Epoch time: 122.56 s 
2025-08-16 14:34:02.355879:  
2025-08-16 14:34:02.355961: Epoch 978 
2025-08-16 14:34:02.356013: Current learning rate: 0.00032 
2025-08-16 14:36:04.869823: train_loss -0.8959 
2025-08-16 14:36:04.869921: val_loss -0.9028 
2025-08-16 14:36:04.869947: Pseudo dice [np.float32(0.935), np.float32(0.967)] 
2025-08-16 14:36:04.869976: Epoch time: 122.52 s 
2025-08-16 14:36:05.337457:  
2025-08-16 14:36:05.337532: Epoch 979 
2025-08-16 14:36:05.337585: Current learning rate: 0.00031 
2025-08-16 14:38:07.952388: train_loss -0.8884 
2025-08-16 14:38:07.952501: val_loss -0.9027 
2025-08-16 14:38:07.952527: Pseudo dice [np.float32(0.9309), np.float32(0.9649)] 
2025-08-16 14:38:07.952558: Epoch time: 122.62 s 
2025-08-16 14:38:08.425095:  
2025-08-16 14:38:08.425192: Epoch 980 
2025-08-16 14:38:08.425245: Current learning rate: 0.0003 
2025-08-16 14:40:15.142777: train_loss -0.892 
2025-08-16 14:40:15.142861: val_loss -0.9083 
2025-08-16 14:40:15.142887: Pseudo dice [np.float32(0.9282), np.float32(0.9681)] 
2025-08-16 14:40:15.142917: Epoch time: 126.72 s 
2025-08-16 14:40:15.617995:  
2025-08-16 14:40:15.618065: Epoch 981 
2025-08-16 14:40:15.618116: Current learning rate: 0.00028 
2025-08-16 14:42:18.457353: train_loss -0.8996 
2025-08-16 14:42:18.457462: val_loss -0.9103 
2025-08-16 14:42:18.457489: Pseudo dice [np.float32(0.9354), np.float32(0.9685)] 
2025-08-16 14:42:18.457522: Epoch time: 122.84 s 
2025-08-16 14:42:18.924120:  
2025-08-16 14:42:18.924202: Epoch 982 
2025-08-16 14:42:18.924256: Current learning rate: 0.00027 
2025-08-16 14:44:21.824147: train_loss -0.8991 
2025-08-16 14:44:21.824262: val_loss -0.9122 
2025-08-16 14:44:21.824308: Pseudo dice [np.float32(0.9309), np.float32(0.968)] 
2025-08-16 14:44:21.824341: Epoch time: 122.9 s 
2025-08-16 14:44:22.290007:  
2025-08-16 14:44:22.290097: Epoch 983 
2025-08-16 14:44:22.290147: Current learning rate: 0.00026 
2025-08-16 14:46:24.801758: train_loss -0.8948 
2025-08-16 14:46:24.801850: val_loss -0.9113 
2025-08-16 14:46:24.801880: Pseudo dice [np.float32(0.934), np.float32(0.9668)] 
2025-08-16 14:46:24.801920: Epoch time: 122.51 s 
2025-08-16 14:46:25.267298:  
2025-08-16 14:46:25.267376: Epoch 984 
2025-08-16 14:46:25.267426: Current learning rate: 0.00024 
2025-08-16 14:48:27.843780: train_loss -0.8974 
2025-08-16 14:48:27.843872: val_loss -0.9163 
2025-08-16 14:48:27.843899: Pseudo dice [np.float32(0.9369), np.float32(0.9665)] 
2025-08-16 14:48:27.843936: Epoch time: 122.58 s 
2025-08-16 14:48:28.309104:  
2025-08-16 14:48:28.309203: Epoch 985 
2025-08-16 14:48:28.309252: Current learning rate: 0.00023 
2025-08-16 14:50:30.888292: train_loss -0.901 
2025-08-16 14:50:30.888386: val_loss -0.9189 
2025-08-16 14:50:30.888413: Pseudo dice [np.float32(0.9373), np.float32(0.9642)] 
2025-08-16 14:50:30.888470: Epoch time: 122.58 s 
2025-08-16 14:50:31.713634:  
2025-08-16 14:50:31.713710: Epoch 986 
2025-08-16 14:50:31.713765: Current learning rate: 0.00021 
2025-08-16 14:52:36.009874: train_loss -0.8909 
2025-08-16 14:52:36.009983: val_loss -0.9105 
2025-08-16 14:52:36.010008: Pseudo dice [np.float32(0.9315), np.float32(0.9663)] 
2025-08-16 14:52:36.010040: Epoch time: 124.3 s 
2025-08-16 14:52:36.474271:  
2025-08-16 14:52:36.474351: Epoch 987 
2025-08-16 14:52:36.474401: Current learning rate: 0.0002 
2025-08-16 14:54:39.021042: train_loss -0.8954 
2025-08-16 14:54:39.021145: val_loss -0.9038 
2025-08-16 14:54:39.021173: Pseudo dice [np.float32(0.9273), np.float32(0.9661)] 
2025-08-16 14:54:39.021206: Epoch time: 122.55 s 
2025-08-16 14:54:39.489049:  
2025-08-16 14:54:39.489125: Epoch 988 
2025-08-16 14:54:39.489177: Current learning rate: 0.00019 
2025-08-16 14:56:43.693366: train_loss -0.8893 
2025-08-16 14:56:43.693477: val_loss -0.9005 
2025-08-16 14:56:43.693503: Pseudo dice [np.float32(0.9349), np.float32(0.9666)] 
2025-08-16 14:56:43.693536: Epoch time: 124.21 s 
2025-08-16 14:56:44.171079:  
2025-08-16 14:56:44.171176: Epoch 989 
2025-08-16 14:56:44.171226: Current learning rate: 0.00017 
2025-08-16 14:58:46.698071: train_loss -0.9002 
2025-08-16 14:58:46.698172: val_loss -0.9168 
2025-08-16 14:58:46.698215: Pseudo dice [np.float32(0.9351), np.float32(0.9687)] 
2025-08-16 14:58:46.698250: Epoch time: 122.53 s 
2025-08-16 14:58:47.174379:  
2025-08-16 14:58:47.174471: Epoch 990 
2025-08-16 14:58:47.174522: Current learning rate: 0.00016 
2025-08-16 15:00:49.754266: train_loss -0.8948 
2025-08-16 15:00:49.754367: val_loss -0.9129 
2025-08-16 15:00:49.754393: Pseudo dice [np.float32(0.9314), np.float32(0.9664)] 
2025-08-16 15:00:49.754425: Epoch time: 122.58 s 
2025-08-16 15:00:50.230836:  
2025-08-16 15:00:50.230920: Epoch 991 
2025-08-16 15:00:50.230973: Current learning rate: 0.00014 
2025-08-16 15:02:52.935662: train_loss -0.8954 
2025-08-16 15:02:52.935765: val_loss -0.9108 
2025-08-16 15:02:52.935792: Pseudo dice [np.float32(0.9314), np.float32(0.9648)] 
2025-08-16 15:02:52.935825: Epoch time: 122.71 s 
2025-08-16 15:02:53.405578:  
2025-08-16 15:02:53.405657: Epoch 992 
2025-08-16 15:02:53.405709: Current learning rate: 0.00013 
2025-08-16 15:04:57.015958: train_loss -0.8957 
2025-08-16 15:04:57.016053: val_loss -0.9121 
2025-08-16 15:04:57.016079: Pseudo dice [np.float32(0.9318), np.float32(0.9656)] 
2025-08-16 15:04:57.016113: Epoch time: 123.61 s 
2025-08-16 15:04:57.483521:  
2025-08-16 15:04:57.483596: Epoch 993 
2025-08-16 15:04:57.483671: Current learning rate: 0.00011 
2025-08-16 15:07:00.017581: train_loss -0.8901 
2025-08-16 15:07:00.017691: val_loss -0.9196 
2025-08-16 15:07:00.017729: Pseudo dice [np.float32(0.9348), np.float32(0.9678)] 
2025-08-16 15:07:00.017763: Epoch time: 122.54 s 
2025-08-16 15:07:00.482975:  
2025-08-16 15:07:00.483049: Epoch 994 
2025-08-16 15:07:00.483123: Current learning rate: 0.0001 
2025-08-16 15:09:03.058621: train_loss -0.8933 
2025-08-16 15:09:03.058719: val_loss -0.9198 
2025-08-16 15:09:03.058746: Pseudo dice [np.float32(0.9323), np.float32(0.9689)] 
2025-08-16 15:09:03.058777: Epoch time: 122.58 s 
2025-08-16 15:09:03.523179:  
2025-08-16 15:09:03.523257: Epoch 995 
2025-08-16 15:09:03.523311: Current learning rate: 8e-05 
2025-08-16 15:11:06.107958: train_loss -0.9005 
2025-08-16 15:11:06.108077: val_loss -0.9211 
2025-08-16 15:11:06.108105: Pseudo dice [np.float32(0.9287), np.float32(0.966)] 
2025-08-16 15:11:06.108138: Epoch time: 122.59 s 
2025-08-16 15:11:06.573447:  
2025-08-16 15:11:06.573530: Epoch 996 
2025-08-16 15:11:06.573581: Current learning rate: 7e-05 
2025-08-16 15:13:12.175477: train_loss -0.896 
2025-08-16 15:13:12.175570: val_loss -0.9087 
2025-08-16 15:13:12.175596: Pseudo dice [np.float32(0.9297), np.float32(0.9676)] 
2025-08-16 15:13:12.175626: Epoch time: 125.6 s 
2025-08-16 15:13:12.642004:  
2025-08-16 15:13:12.642074: Epoch 997 
2025-08-16 15:13:12.642125: Current learning rate: 5e-05 
2025-08-16 15:15:15.237901: train_loss -0.893 
2025-08-16 15:15:15.238017: val_loss -0.9217 
2025-08-16 15:15:15.238044: Pseudo dice [np.float32(0.9317), np.float32(0.9685)] 
2025-08-16 15:15:15.238077: Epoch time: 122.6 s 
2025-08-16 15:15:15.713227:  
2025-08-16 15:15:15.713313: Epoch 998 
2025-08-16 15:15:15.713367: Current learning rate: 4e-05 
2025-08-16 15:17:18.266326: train_loss -0.8979 
2025-08-16 15:17:18.266419: val_loss -0.9232 
2025-08-16 15:17:18.266446: Pseudo dice [np.float32(0.9344), np.float32(0.9669)] 
2025-08-16 15:17:18.266478: Epoch time: 122.55 s 
2025-08-16 15:17:18.731499:  
2025-08-16 15:17:18.731578: Epoch 999 
2025-08-16 15:17:18.731631: Current learning rate: 2e-05 
2025-08-16 15:19:21.345601: train_loss -0.8961 
2025-08-16 15:19:21.345703: val_loss -0.9234 
2025-08-16 15:19:21.345731: Pseudo dice [np.float32(0.9379), np.float32(0.9686)] 
2025-08-16 15:19:21.345762: Epoch time: 122.62 s 
2025-08-16 15:19:22.320238: Training done. 
2025-08-16 15:19:22.326089: predicting train_0001 
2025-08-16 15:19:22.402266: train_0001, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:19:36.724919: predicting train_0002 
2025-08-16 15:19:36.827718: train_0002, shape torch.Size([2, 192, 371, 192]), rank 0 
2025-08-16 15:19:38.468269: predicting train_0003 
2025-08-16 15:19:38.571190: train_0003, shape torch.Size([2, 223, 300, 223]), rank 0 
2025-08-16 15:19:45.020490: predicting train_0004 
2025-08-16 15:19:45.076391: train_0004, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:19:46.736220: predicting train_0005 
2025-08-16 15:19:46.792528: train_0005, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:19:48.449789: predicting train_0006 
2025-08-16 15:19:48.524951: train_0006, shape torch.Size([2, 188, 297, 188]), rank 0 
2025-08-16 15:19:50.184410: predicting train_0007 
2025-08-16 15:19:50.240707: train_0007, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:19:51.899350: predicting train_0008 
2025-08-16 15:19:51.985031: train_0008, shape torch.Size([2, 192, 335, 192]), rank 0 
2025-08-16 15:19:53.622235: predicting train_0009 
2025-08-16 15:19:53.713058: train_0009, shape torch.Size([2, 158, 440, 158]), rank 0 
2025-08-16 15:19:55.920374: predicting train_0010 
2025-08-16 15:19:56.036111: train_0010, shape torch.Size([2, 223, 336, 223]), rank 0 
2025-08-16 15:20:02.487571: predicting train_0011 
2025-08-16 15:20:02.630383: train_0011, shape torch.Size([2, 223, 405, 223]), rank 0 
2025-08-16 15:20:11.224968: predicting train_0012 
2025-08-16 15:20:11.277031: train_0012, shape torch.Size([2, 151, 299, 151]), rank 0 
2025-08-16 15:20:12.928079: predicting train_0013 
2025-08-16 15:20:13.038301: train_0013, shape torch.Size([2, 223, 310, 223]), rank 0 
2025-08-16 15:20:19.484951: predicting train_0014 
2025-08-16 15:20:19.590600: train_0014, shape torch.Size([2, 223, 308, 223]), rank 0 
2025-08-16 15:20:26.041722: predicting train_0015 
2025-08-16 15:20:26.092009: train_0015, shape torch.Size([2, 151, 299, 151]), rank 0 
2025-08-16 15:20:27.742896: predicting train_0016 
2025-08-16 15:20:27.793794: train_0016, shape torch.Size([2, 151, 299, 151]), rank 0 
2025-08-16 15:20:29.446802: predicting train_0017 
2025-08-16 15:20:29.569387: train_0017, shape torch.Size([2, 223, 339, 223]), rank 0 
2025-08-16 15:20:36.025178: predicting train_0018 
2025-08-16 15:20:36.076570: train_0018, shape torch.Size([2, 158, 286, 158]), rank 0 
2025-08-16 15:20:37.198441: predicting train_0019 
2025-08-16 15:20:37.252791: train_0019, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:20:38.909202: predicting train_0020 
2025-08-16 15:20:38.964529: train_0020, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:20:40.620973: predicting train_0021 
2025-08-16 15:20:40.672016: train_0021, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:20:42.329418: predicting train_0022 
2025-08-16 15:20:42.417336: train_0022, shape torch.Size([2, 192, 335, 192]), rank 0 
2025-08-16 15:20:44.055205: predicting train_0023 
2025-08-16 15:20:44.110869: train_0023, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:20:45.768928: predicting train_0024 
2025-08-16 15:20:45.887477: train_0024, shape torch.Size([2, 223, 350, 223]), rank 0 
2025-08-16 15:20:52.338351: predicting train_0025 
2025-08-16 15:20:52.442993: train_0025, shape torch.Size([2, 223, 322, 223]), rank 0 
2025-08-16 15:20:58.895275: predicting train_0026 
2025-08-16 15:20:58.952066: train_0026, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:21:00.609072: predicting train_0027 
2025-08-16 15:21:00.800804: train_0027, shape torch.Size([2, 203, 578, 203]), rank 0 
2025-08-16 15:21:13.681499: predicting train_0028 
2025-08-16 15:21:13.742013: train_0028, shape torch.Size([2, 151, 299, 151]), rank 0 
2025-08-16 15:21:15.393104: predicting train_0029 
2025-08-16 15:21:15.495130: train_0029, shape torch.Size([2, 223, 306, 223]), rank 0 
2025-08-16 15:21:21.941126: predicting train_0030 
2025-08-16 15:21:22.083166: train_0030, shape torch.Size([2, 223, 405, 223]), rank 0 
2025-08-16 15:21:30.678272: predicting train_0031 
2025-08-16 15:21:30.835583: train_0031, shape torch.Size([2, 203, 528, 203]), rank 0 
2025-08-16 15:21:41.567285: predicting train_0032 
2025-08-16 15:21:41.619537: train_0032, shape torch.Size([2, 151, 299, 151]), rank 0 
2025-08-16 15:21:43.271932: predicting train_0033 
2025-08-16 15:21:43.384402: train_0033, shape torch.Size([2, 223, 330, 223]), rank 0 
2025-08-16 15:21:49.835380: predicting train_0034 
2025-08-16 15:21:49.890589: train_0034, shape torch.Size([2, 158, 312, 158]), rank 0 
2025-08-16 15:21:51.546667: predicting train_0035 
2025-08-16 15:21:51.651489: train_0035, shape torch.Size([2, 223, 298, 223]), rank 0 
2025-08-16 15:21:58.095792: predicting train_0036 
2025-08-16 15:21:58.152274: train_0036, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:21:59.813519: predicting train_0037 
2025-08-16 15:21:59.923017: train_0037, shape torch.Size([2, 192, 383, 192]), rank 0 
2025-08-16 15:22:01.564046: predicting train_0038 
2025-08-16 15:22:01.630600: train_0038, shape torch.Size([2, 151, 407, 151]), rank 0 
2025-08-16 15:22:03.830260: predicting train_0039 
2025-08-16 15:22:03.924188: train_0039, shape torch.Size([2, 223, 284, 223]), rank 0 
2025-08-16 15:22:08.236722: predicting train_0040 
2025-08-16 15:22:08.350391: train_0040, shape torch.Size([2, 223, 328, 223]), rank 0 
2025-08-16 15:22:14.804661: predicting train_0041 
2025-08-16 15:22:14.908927: train_0041, shape torch.Size([2, 223, 309, 223]), rank 0 
2025-08-16 15:22:21.363600: predicting train_0042 
2025-08-16 15:22:21.465451: train_0042, shape torch.Size([2, 223, 323, 223]), rank 0 
2025-08-16 15:22:27.921007: predicting train_0043 
2025-08-16 15:22:28.018917: train_0043, shape torch.Size([2, 203, 323, 203]), rank 0 
2025-08-16 15:22:34.463966: predicting train_0044 
2025-08-16 15:22:34.565618: train_0044, shape torch.Size([2, 223, 298, 223]), rank 0 
2025-08-16 15:22:41.019532: predicting train_0045 
2025-08-16 15:22:41.072233: train_0045, shape torch.Size([2, 151, 299, 151]), rank 0 
2025-08-16 15:22:42.727760: predicting train_0046 
2025-08-16 15:22:42.782564: train_0046, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:22:44.437905: predicting train_0047 
2025-08-16 15:22:44.546427: train_0047, shape torch.Size([2, 223, 315, 223]), rank 0 
2025-08-16 15:22:50.994436: predicting train_0048 
2025-08-16 15:22:51.050894: train_0048, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:22:52.713230: predicting train_0049 
2025-08-16 15:22:52.809289: train_0049, shape torch.Size([2, 223, 296, 223]), rank 0 
2025-08-16 15:22:59.253027: predicting train_0050 
2025-08-16 15:22:59.307943: train_0050, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:23:00.965284: predicting train_0051 
2025-08-16 15:23:01.036702: train_0051, shape torch.Size([2, 156, 389, 156]), rank 0 
2025-08-16 15:23:03.237013: predicting train_0052 
2025-08-16 15:23:03.292342: train_0052, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:23:04.949556: predicting train_0053 
2025-08-16 15:23:05.054528: train_0053, shape torch.Size([2, 223, 306, 223]), rank 0 
2025-08-16 15:23:11.500662: predicting train_0054 
2025-08-16 15:23:11.610450: train_0054, shape torch.Size([2, 223, 307, 223]), rank 0 
2025-08-16 15:23:18.062463: predicting train_0055 
2025-08-16 15:23:18.131841: train_0055, shape torch.Size([2, 151, 407, 151]), rank 0 
2025-08-16 15:23:20.334826: predicting train_0056 
2025-08-16 15:23:20.459185: train_0056, shape torch.Size([2, 203, 416, 203]), rank 0 
2025-08-16 15:23:29.049342: predicting train_0057 
2025-08-16 15:23:29.106887: train_0057, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:23:30.763639: predicting train_0058 
2025-08-16 15:23:30.866647: train_0058, shape torch.Size([2, 192, 383, 192]), rank 0 
2025-08-16 15:23:32.509852: predicting train_0059 
2025-08-16 15:23:32.565171: train_0059, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:23:34.221507: predicting train_0060 
2025-08-16 15:23:34.321052: train_0060, shape torch.Size([2, 223, 298, 223]), rank 0 
2025-08-16 15:23:40.767069: predicting train_0061 
2025-08-16 15:23:40.832424: train_0061, shape torch.Size([2, 151, 371, 151]), rank 0 
2025-08-16 15:23:42.496502: predicting train_0062 
2025-08-16 15:23:42.597968: train_0062, shape torch.Size([2, 223, 298, 223]), rank 0 
2025-08-16 15:23:49.044047: predicting train_0063 
2025-08-16 15:23:49.122210: train_0063, shape torch.Size([2, 185, 312, 185]), rank 0 
2025-08-16 15:23:50.787283: predicting train_0064 
2025-08-16 15:23:50.902276: train_0064, shape torch.Size([2, 192, 429, 192]), rank 0 
2025-08-16 15:23:53.080831: predicting train_0065 
2025-08-16 15:23:53.194746: train_0065, shape torch.Size([2, 203, 323, 203]), rank 0 
2025-08-16 15:23:59.641021: predicting train_0066 
2025-08-16 15:23:59.698774: train_0066, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:24:01.359723: predicting train_0067 
2025-08-16 15:24:01.416443: train_0067, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:24:03.072152: predicting train_0068 
2025-08-16 15:24:03.174032: train_0068, shape torch.Size([2, 223, 296, 223]), rank 0 
2025-08-16 15:24:09.619151: predicting train_0069 
2025-08-16 15:24:09.688059: train_0069, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:24:11.347581: predicting train_0070 
2025-08-16 15:24:11.399222: train_0070, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:24:13.055586: predicting train_0071 
2025-08-16 15:24:13.122591: train_0071, shape torch.Size([2, 151, 407, 151]), rank 0 
2025-08-16 15:24:15.322492: predicting train_0072 
2025-08-16 15:24:15.444204: train_0072, shape torch.Size([2, 223, 350, 223]), rank 0 
2025-08-16 15:24:21.896570: predicting train_0073 
2025-08-16 15:24:21.954732: train_0073, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:24:23.616030: predicting train_0074 
2025-08-16 15:24:23.883886: train_0074, shape torch.Size([2, 363, 437, 363]), rank 0 
2025-08-16 15:24:43.228866: predicting train_0075 
2025-08-16 15:24:43.281188: train_0075, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:24:44.945075: predicting train_0076 
2025-08-16 15:24:44.996475: train_0076, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:24:46.653010: predicting train_0077 
2025-08-16 15:24:46.733610: train_0077, shape torch.Size([2, 192, 335, 192]), rank 0 
2025-08-16 15:24:48.371193: predicting train_0078 
2025-08-16 15:24:48.467470: train_0078, shape torch.Size([2, 158, 440, 158]), rank 0 
2025-08-16 15:24:50.672593: predicting train_0079 
2025-08-16 15:24:50.783976: train_0079, shape torch.Size([2, 223, 330, 223]), rank 0 
2025-08-16 15:24:57.235848: predicting train_0080 
2025-08-16 15:24:57.345511: train_0080, shape torch.Size([2, 223, 312, 223]), rank 0 
2025-08-16 15:25:03.794704: predicting train_0081 
2025-08-16 15:25:03.901028: train_0081, shape torch.Size([2, 223, 300, 223]), rank 0 
2025-08-16 15:25:10.350444: predicting train_0082 
2025-08-16 15:25:10.402266: train_0082, shape torch.Size([2, 151, 299, 151]), rank 0 
2025-08-16 15:25:12.056423: predicting train_0083 
2025-08-16 15:25:12.189830: train_0083, shape torch.Size([2, 223, 342, 223]), rank 0 
2025-08-16 15:25:18.640086: predicting train_0084 
2025-08-16 15:25:18.744428: train_0084, shape torch.Size([2, 223, 296, 223]), rank 0 
2025-08-16 15:25:25.195582: predicting train_0085 
2025-08-16 15:25:25.266707: train_0085, shape torch.Size([2, 151, 371, 151]), rank 0 
2025-08-16 15:25:26.932499: predicting train_0086 
2025-08-16 15:25:27.007565: train_0086, shape torch.Size([2, 192, 335, 192]), rank 0 
2025-08-16 15:25:28.645155: predicting train_0087 
2025-08-16 15:25:28.760783: train_0087, shape torch.Size([2, 223, 336, 223]), rank 0 
2025-08-16 15:25:35.210811: predicting train_0088 
2025-08-16 15:25:35.299381: train_0088, shape torch.Size([2, 185, 338, 185]), rank 0 
2025-08-16 15:25:36.962139: predicting train_0089 
2025-08-16 15:25:37.064733: train_0089, shape torch.Size([2, 223, 298, 223]), rank 0 
2025-08-16 15:25:43.516452: predicting train_0090 
2025-08-16 15:25:43.638527: train_0090, shape torch.Size([2, 223, 350, 223]), rank 0 
2025-08-16 15:25:50.091961: predicting train_0091 
2025-08-16 15:25:50.207040: train_0091, shape torch.Size([2, 192, 429, 192]), rank 0 
2025-08-16 15:25:52.388906: predicting train_0092 
2025-08-16 15:25:52.490172: train_0092, shape torch.Size([2, 223, 296, 223]), rank 0 
2025-08-16 15:25:58.937872: predicting train_0093 
2025-08-16 15:25:59.042553: train_0093, shape torch.Size([2, 223, 298, 223]), rank 0 
2025-08-16 15:26:05.487880: predicting train_0094 
2025-08-16 15:26:05.593314: train_0094, shape torch.Size([2, 223, 298, 223]), rank 0 
2025-08-16 15:26:12.040676: predicting train_0095 
2025-08-16 15:26:12.139671: train_0095, shape torch.Size([2, 223, 285, 223]), rank 0 
2025-08-16 15:26:16.451183: predicting train_0096 
2025-08-16 15:26:16.502850: train_0096, shape torch.Size([2, 151, 299, 151]), rank 0 
2025-08-16 15:26:18.157782: predicting train_0097 
2025-08-16 15:26:18.260057: train_0097, shape torch.Size([2, 223, 298, 223]), rank 0 
2025-08-16 15:26:24.704176: predicting train_0098 
2025-08-16 15:26:24.768287: train_0098, shape torch.Size([2, 151, 371, 151]), rank 0 
2025-08-16 15:26:26.431757: predicting train_0099 
2025-08-16 15:26:26.480236: train_0099, shape torch.Size([2, 151, 299, 151]), rank 0 
2025-08-16 15:26:28.131745: predicting train_0100 
2025-08-16 15:26:28.182222: train_0100, shape torch.Size([2, 151, 299, 151]), rank 0 
2025-08-16 15:26:29.833573: predicting train_1001 
2025-08-16 15:26:29.884855: train_1001, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:26:31.539656: predicting train_1002 
2025-08-16 15:26:31.639360: train_1002, shape torch.Size([2, 192, 371, 192]), rank 0 
2025-08-16 15:26:33.278648: predicting train_1003 
2025-08-16 15:26:33.468800: train_1003, shape torch.Size([2, 223, 551, 223]), rank 0 
2025-08-16 15:26:44.222985: predicting train_1004 
2025-08-16 15:26:44.313319: train_1004, shape torch.Size([2, 192, 335, 192]), rank 0 
2025-08-16 15:26:45.951976: predicting train_1005 
2025-08-16 15:26:46.005618: train_1005, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:26:47.663629: predicting train_1006 
2025-08-16 15:26:47.767133: train_1006, shape torch.Size([2, 223, 297, 223]), rank 0 
2025-08-16 15:26:54.211986: predicting train_1007 
2025-08-16 15:26:54.265855: train_1007, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:26:55.924260: predicting train_1008 
2025-08-16 15:26:55.973598: train_1008, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:26:57.631207: predicting train_1009 
2025-08-16 15:26:57.739850: train_1009, shape torch.Size([2, 185, 440, 185]), rank 0 
2025-08-16 15:26:59.953162: predicting train_1010 
2025-08-16 15:27:00.054647: train_1010, shape torch.Size([2, 223, 290, 223]), rank 0 
2025-08-16 15:27:06.501647: predicting train_1011 
2025-08-16 15:27:06.633933: train_1011, shape torch.Size([2, 223, 378, 223]), rank 0 
2025-08-16 15:27:13.091570: predicting train_1012 
2025-08-16 15:27:13.140772: train_1012, shape torch.Size([2, 151, 299, 151]), rank 0 
2025-08-16 15:27:14.797006: predicting train_1013 
2025-08-16 15:27:14.906270: train_1013, shape torch.Size([2, 223, 317, 223]), rank 0 
2025-08-16 15:27:21.354822: predicting train_1014 
2025-08-16 15:27:21.448527: train_1014, shape torch.Size([2, 223, 277, 223]), rank 0 
2025-08-16 15:27:25.760405: predicting train_1015 
2025-08-16 15:27:25.804418: train_1015, shape torch.Size([2, 151, 299, 151]), rank 0 
2025-08-16 15:27:27.454793: predicting train_1016 
2025-08-16 15:27:27.510545: train_1016, shape torch.Size([2, 151, 299, 151]), rank 0 
2025-08-16 15:27:29.162270: predicting train_1017 
2025-08-16 15:27:29.279994: train_1017, shape torch.Size([2, 223, 339, 223]), rank 0 
2025-08-16 15:27:35.729343: predicting train_1018 
2025-08-16 15:27:35.774789: train_1018, shape torch.Size([2, 158, 286, 158]), rank 0 
2025-08-16 15:27:36.896207: predicting train_1019 
2025-08-16 15:27:36.945030: train_1019, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:27:38.600243: predicting train_1020 
2025-08-16 15:27:38.646442: train_1020, shape torch.Size([2, 151, 299, 151]), rank 0 
2025-08-16 15:27:40.298118: predicting train_1021 
2025-08-16 15:27:40.350520: train_1021, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:27:42.006128: predicting train_1022 
2025-08-16 15:27:42.111231: train_1022, shape torch.Size([2, 192, 407, 192]), rank 0 
2025-08-16 15:27:44.287851: predicting train_1023 
2025-08-16 15:27:44.375217: train_1023, shape torch.Size([2, 192, 335, 192]), rank 0 
2025-08-16 15:27:46.015461: predicting train_1024 
2025-08-16 15:27:46.135401: train_1024, shape torch.Size([2, 223, 350, 223]), rank 0 
2025-08-16 15:27:52.589515: predicting train_1025 
2025-08-16 15:27:52.774264: train_1025, shape torch.Size([2, 223, 558, 223]), rank 0 
2025-08-16 15:28:03.520894: predicting train_1026 
2025-08-16 15:28:03.566110: train_1026, shape torch.Size([2, 151, 299, 151]), rank 0 
2025-08-16 15:28:05.221109: predicting train_1027 
2025-08-16 15:28:05.323358: train_1027, shape torch.Size([2, 203, 343, 203]), rank 0 
2025-08-16 15:28:11.767237: predicting train_1028 
2025-08-16 15:28:11.822716: train_1028, shape torch.Size([2, 156, 297, 156]), rank 0 
2025-08-16 15:28:13.471938: predicting train_1029 
2025-08-16 15:28:13.568006: train_1029, shape torch.Size([2, 223, 311, 223]), rank 0 
2025-08-16 15:28:20.020379: predicting train_1030 
2025-08-16 15:28:20.162652: train_1030, shape torch.Size([2, 223, 405, 223]), rank 0 
2025-08-16 15:28:28.763977: predicting train_1031 
2025-08-16 15:28:28.928777: train_1031, shape torch.Size([2, 203, 553, 203]), rank 0 
2025-08-16 15:28:39.671953: predicting train_1032 
2025-08-16 15:28:39.727418: train_1032, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:28:41.383276: predicting train_1033 
2025-08-16 15:28:41.487447: train_1033, shape torch.Size([2, 223, 301, 223]), rank 0 
2025-08-16 15:28:47.935725: predicting train_1034 
2025-08-16 15:28:47.987103: train_1034, shape torch.Size([2, 158, 312, 158]), rank 0 
2025-08-16 15:28:49.646634: predicting train_1035 
2025-08-16 15:28:49.748880: train_1035, shape torch.Size([2, 223, 298, 223]), rank 0 
2025-08-16 15:28:56.194727: predicting train_1036 
2025-08-16 15:28:56.284796: train_1036, shape torch.Size([2, 192, 335, 192]), rank 0 
2025-08-16 15:28:57.925992: predicting train_1037 
2025-08-16 15:28:58.027845: train_1037, shape torch.Size([2, 192, 383, 192]), rank 0 
2025-08-16 15:28:59.670409: predicting train_1038 
2025-08-16 15:28:59.787303: train_1038, shape torch.Size([2, 192, 407, 192]), rank 0 
2025-08-16 15:29:01.962044: predicting train_1039 
2025-08-16 15:29:02.063075: train_1039, shape torch.Size([2, 223, 301, 223]), rank 0 
2025-08-16 15:29:08.513546: predicting train_1040 
2025-08-16 15:29:08.618767: train_1040, shape torch.Size([2, 223, 339, 223]), rank 0 
2025-08-16 15:29:15.073668: predicting train_1041 
2025-08-16 15:29:15.182293: train_1041, shape torch.Size([2, 223, 309, 223]), rank 0 
2025-08-16 15:29:21.634325: predicting train_1042 
2025-08-16 15:29:21.738204: train_1042, shape torch.Size([2, 192, 383, 192]), rank 0 
2025-08-16 15:29:23.382304: predicting train_1043 
2025-08-16 15:29:23.435462: train_1043, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:29:25.090852: predicting train_1044 
2025-08-16 15:29:25.207251: train_1044, shape torch.Size([2, 223, 338, 223]), rank 0 
2025-08-16 15:29:31.656488: predicting train_1045 
2025-08-16 15:29:31.712301: train_1045, shape torch.Size([2, 151, 335, 151]), rank 0 
2025-08-16 15:29:33.372697: predicting train_1046 
2025-08-16 15:29:33.456733: train_1046, shape torch.Size([2, 192, 335, 192]), rank 0 
2025-08-16 15:29:35.094389: predicting train_1047 
2025-08-16 15:29:35.218466: train_1047, shape torch.Size([2, 223, 352, 223]), rank 0 
